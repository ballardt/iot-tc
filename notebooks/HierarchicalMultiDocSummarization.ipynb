{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Hierarchical Multi-Document Summarization\n",
    "\n",
    "Basic pipeline:\n",
    "1. Vectorize all documents. Each doc is a cluster at the start of HAC\n",
    "2. Merge the clusters with the highest cosine similarity between them. Because this is HAC, we may\n",
    "   use \"complete\", \"average\", or another method.\n",
    "3. Pick candidate summaries by mixing and matching sentences from each document. Picking good\n",
    "   candidates (i.e. not the Cartesian product) is non-trivial, and one of many existing methods\n",
    "   in the literature may be used. Vectorize the candidate summaries.\n",
    "4. Pick the summary with the highest cosine similarity to the cluster average.\n",
    "5. Repeat 2-4 until all clusters have merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Preliminary\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim as g\n",
    "\n",
    "BASE_PATH = '/home/trevor/Projects/iot-diff/'\n",
    "D2V_APNEWS_PATH = os.path.join(BASE_PATH, 'iot-tc/apnews_dbow/doc2vec.bin')\n",
    "DATA_CNET_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/cnet-classifications.csv')\n",
    "DATA_DUC2006_RAW_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2006/duc2006_docs')\n",
    "DATA_DUC2007_RAW_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2007/duc2007_testdocs/main')\n",
    "CORENLP_PATH = os.path.join(BASE_PATH, 'stanford-corenlp/')\n",
    "\n",
    "def clean_duc_text(raw_text):\n",
    "    content = re.split('<\\/?TEXT>', raw_text)[1]\n",
    "    clean_content = '\\n'.join(re.split('<\\/?(?:.*?)>', content))\n",
    "    return clean_content\n",
    "\n",
    "def load_duc_data(raw_path):\n",
    "    data = {\n",
    "         'topic': [],\n",
    "         'document': [],\n",
    "         'content': []\n",
    "    }\n",
    "    for root, dirs, files in os.walk(raw_path):\n",
    "        # Ignore the top directory\n",
    "        if files:\n",
    "            topic = os.path.basename(os.path.normpath(root))\n",
    "            for name in files:\n",
    "                document = name\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    content = clean_duc_text(f.read())\n",
    "                    data['topic'].append(topic)\n",
    "                    data['document'].append(document)\n",
    "                    data['content'].append(content)\n",
    "    duc_df = pd.DataFrame.from_dict(data)\n",
    "    return duc_df\n",
    "\n",
    "doc2vec = g.models.doc2vec.Doc2Vec.load(D2V_APNEWS_PATH)\n",
    "cnet_df = pd.read_csv(DATA_CNET_PATH)\n",
    "duc2006_df = load_duc_data(DATA_DUC2006_RAW_PATH)\n",
    "duc2007_df = load_duc_data(DATA_DUC2007_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize all documents. Each document represents a cluster at the start of HAC.\n",
    "import stanfordcorenlp as corenlp\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "def preprocess(text):\n",
    "    # tokenize\n",
    "    nlp = corenlp.StanfordCoreNLP(os.path.join(CORENLP_PATH))\n",
    "    tokens = nlp.word_tokenize(text)\n",
    "    # remove punctuation\n",
    "    tokens = [token for token in tokens if any(c.isalnum() for c in token)]\n",
    "    # lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # remove stop words\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    nlp.close()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DUC 2007\n",
      "Iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1100\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# CNET\n",
    "print('CNET')\n",
    "cnet_docvecs = []\n",
    "for i, row in cnet_df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "       print('Iteration {}'.format(i))\n",
    "    text = row['article_content']\n",
    "    cnet_docvecs.append(doc2vec.infer_vector(preprocess(text)))\n",
    "with open('d2v_apnews_cnet.pkl', 'wb') as f:\n",
    "     pickle.dump(cnet_docvecs, f)\n",
    "\n",
    "# DUC 2006\n",
    "print('\\nDUC 2006')\n",
    "duc2006_docvecs = []\n",
    "for i, row in duc2006_df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "       print('Iteration {}'.format(i))\n",
    "    text = row['content']\n",
    "    duc2006_docvecs.append(doc2vec.infer_vector(preprocess(text)))\n",
    "with open('d2v_apnews_duc2006.pkl', 'wb') as f:\n",
    "     pickle.dump(duc2006_docvecs, f)\n",
    "\n",
    "# DUC 2007\n",
    "print('\\nDUC 2007')\n",
    "duc2007_docvecs = []\n",
    "for i, row in duc2007_df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "       print('Iteration {}'.format(i))\n",
    "    text = row['content']\n",
    "    duc2007_docvecs.append(doc2vec.infer_vector(preprocess(text)))\n",
    "with open('d2v_apnews_duc2007.pkl', 'wb') as f:\n",
    "     pickle.dump(duc2007_docvecs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-computed DataFrames\n",
    "import pickle\n",
    "\n",
    "with open('../d2v_apnews_cnet.pkl', 'rb') as f:\n",
    "     cnet_docvecs = pickle.load(f)\n",
    "with open('d2v_apnews_duc2006.pkl', 'rb') as f:\n",
    "     duc2006_docvecs = pickle.load(f)\n",
    "with open('d2v_apnews_duc2007.pkl', 'rb') as f:\n",
    "     duc2007_docvecs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6081460206326501"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform HAC. In this basic version, HAC is uninformed by summarization\n",
    "# linkage_matrix is an Nx4 matrix, where each column is:\n",
    "#   1) Constituent cluster 1\n",
    "#   2) Constituent cluster 2\n",
    "#   3) Distance between 1 and 2\n",
    "#   4) Num original observations in resulting cluster\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "X = np.asarray(duc2007_docvecs)\n",
    "linkage_matrix = linkage(X, 'average', 'cosine')\n",
    "\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "c, coph_dists = cophenet(linkage_matrix, pdist(X, 'cosine'))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1100\n"
     ]
    }
   ],
   "source": [
    "# Perform multi-doc summarization at each step\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "\n",
    "df = duc2007_df\n",
    "\n",
    "# TODO placeholders, these are awful\n",
    "def get_candidate_sentences(doc_list):\n",
    "    sentences = []\n",
    "    for d in doc_list:\n",
    "        sentences.extend(d.split('.')[0:3])\n",
    "    return sentences\n",
    "\n",
    "def get_candidate_summaries(sentences):\n",
    "    summaries = ['. '.join([random.choice(sentences) for _ in range(3)]) for _ in range(5)]\n",
    "    return summaries\n",
    "\n",
    "N = len(df)\n",
    "best_summaries = {}\n",
    "clusters = {i: [i] for i in range(N)}\n",
    "for i, row in enumerate(linkage_matrix):\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration {}'.format(i))\n",
    "    clusters[N+i] = clusters[row[0]] + clusters[row[1]]\n",
    "    doc_list = [r['content'] for _,r in df.iloc[clusters[N+i]].iterrows()]\n",
    "    candidates = get_candidate_summaries(get_candidate_sentences(doc_list))\n",
    "    # Evaluate each candidate and pick the best one\n",
    "    cluster_avg = np.mean(X[clusters[N+i]], axis=0)\n",
    "    best_summary = {'score': -1, 'summary': ''}\n",
    "    for j, candidate in enumerate(candidates):\n",
    "        candidate_vec = doc2vec.infer_vector(preprocess(candidate))\n",
    "        candidate_score = cosine(cluster_avg, candidate_vec)\n",
    "        if candidate_score > best_summary['score']:\n",
    "            best_summary['score'] = candidate_score\n",
    "            best_summary['summary'] = candidate\n",
    "    best_summaries[i] = {'score': best_summary['score'], 'summary': best_summary['summary']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open('best_summaries_apnews_duc2007.pkl', 'wb') as f:\n",
    "     pickle.dump(best_summaries, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "HierarchicalMultiDocSummarization.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
