{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Hierarchical Multi-Document Summarization\n",
    "\n",
    "Basic pipeline:\n",
    "1. Vectorize all documents. Each doc is a cluster at the start of HAC\n",
    "2. Merge the clusters with the highest cosine similarity between them. Because this is HAC, we may\n",
    "   use \"complete\", \"average\", or another method.\n",
    "3. Pick candidate summaries by mixing and matching sentences from each document. Picking good\n",
    "   candidates (i.e. not the Cartesian product) is non-trivial, and one of many existing methods\n",
    "   in the literature may be used. Vectorize the candidate summaries.\n",
    "4. Pick the summary with the highest cosine similarity to the cluster average.\n",
    "5. Repeat 2-4 until all clusters have merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Preliminary\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim as g\n",
    "\n",
    "BASE_PATH = '/home/trevor/Projects/iot-diff/'\n",
    "D2V_APNEWS_PATH = os.path.join(BASE_PATH, 'iot-tc/apnews_dbow/doc2vec.bin')\n",
    "CNET_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/cnet-classifications.csv')\n",
    "DUC2006_RAW_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2006/duc2006_docs')\n",
    "DUC2006_MODELS_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2006/NISTeval/ROUGE/models')\n",
    "DUC2006_PEERS_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2006/NISTeval/ROUGE/peers')\n",
    "DUC2007_RAW_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2007/duc2007_testdocs/main')\n",
    "DUC2007_MODELS_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2007/mainEval/ROUGE/models')\n",
    "DUC2007_PEERS_PATH = os.path.join(BASE_PATH, 'iot-tc/datasets/duc2007/mainEval/ROUGE/peers')\n",
    "CORENLP_PATH = os.path.join(BASE_PATH, 'stanford-corenlp/')\n",
    "\n",
    "def clean_duc_text(raw_text):\n",
    "    content = re.split('<\\/?TEXT>', raw_text)[1]\n",
    "    clean_content = '\\n'.join(re.split('<\\/?(?:.*?)>', content))\n",
    "    return clean_content\n",
    "\n",
    "def load_duc_data(raw_path):\n",
    "    data = {\n",
    "         'topic': [],\n",
    "         'document': [],\n",
    "         'content': []\n",
    "    }\n",
    "    for root, dirs, files in os.walk(raw_path):\n",
    "        # Ignore the top directory\n",
    "        if files:\n",
    "            topic = os.path.basename(os.path.normpath(root))\n",
    "            for name in files:\n",
    "                document = name\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    content = clean_duc_text(f.read())\n",
    "                    data['topic'].append(topic)\n",
    "                    data['document'].append(document)\n",
    "                    data['content'].append(content)\n",
    "    duc_df = pd.DataFrame.from_dict(data)\n",
    "    return duc_df\n",
    "\n",
    "doc2vec = g.models.doc2vec.Doc2Vec.load(D2V_APNEWS_PATH)\n",
    "cnet_df = pd.read_csv(CNET_PATH)\n",
    "duc2006_df = load_duc_data(DUC2006_RAW_PATH)\n",
    "duc2007_df = load_duc_data(DUC2007_RAW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize all documents. Each document represents a cluster at the start of HAC.\n",
    "import stanfordcorenlp as corenlp\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "def preprocess(text):\n",
    "    # tokenize\n",
    "    nlp = corenlp.StanfordCoreNLP(os.path.join(CORENLP_PATH))\n",
    "    tokens = nlp.word_tokenize(text)\n",
    "    # remove punctuation\n",
    "    tokens = [token for token in tokens if any(c.isalnum() for c in token)]\n",
    "    # lower case\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # remove stop words\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    nlp.close()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DUC 2007\n",
      "Iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1100\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# CNET\n",
    "print('CNET')\n",
    "cnet_docvecs = []\n",
    "for i, row in cnet_df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "       print('Iteration {}'.format(i))\n",
    "    text = row['article_content']\n",
    "    cnet_docvecs.append(doc2vec.infer_vector(preprocess(text)))\n",
    "with open('d2v_apnews_cnet.pkl', 'wb') as f:\n",
    "     pickle.dump(cnet_docvecs, f)\n",
    "\n",
    "# DUC 2006\n",
    "print('\\nDUC 2006')\n",
    "duc2006_docvecs = []\n",
    "for i, row in duc2006_df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "       print('Iteration {}'.format(i))\n",
    "    text = row['content']\n",
    "    duc2006_docvecs.append(doc2vec.infer_vector(preprocess(text)))\n",
    "with open('d2v_apnews_duc2006.pkl', 'wb') as f:\n",
    "     pickle.dump(duc2006_docvecs, f)\n",
    "\n",
    "# DUC 2007\n",
    "print('\\nDUC 2007')\n",
    "duc2007_docvecs = []\n",
    "for i, row in duc2007_df.iterrows():\n",
    "    if i % 100 == 0:\n",
    "       print('Iteration {}'.format(i))\n",
    "    text = row['content']\n",
    "    duc2007_docvecs.append(doc2vec.infer_vector(preprocess(text)))\n",
    "with open('d2v_apnews_duc2007.pkl', 'wb') as f:\n",
    "     pickle.dump(duc2007_docvecs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-computed DataFrames\n",
    "import pickle\n",
    "\n",
    "with open('../d2v_apnews_cnet.pkl', 'rb') as f:\n",
    "     cnet_docvecs = pickle.load(f)\n",
    "with open('d2v_apnews_duc2006.pkl', 'rb') as f:\n",
    "     duc2006_docvecs = pickle.load(f)\n",
    "with open('d2v_apnews_duc2007.pkl', 'rb') as f:\n",
    "     duc2007_docvecs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6081460206326501"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform HAC. In this basic version, HAC is uninformed by summarization\n",
    "# linkage_matrix is an Nx4 matrix, where each column is:\n",
    "#   1) Constituent cluster 1\n",
    "#   2) Constituent cluster 2\n",
    "#   3) Distance between 1 and 2\n",
    "#   4) Num original observations in resulting cluster\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "X = np.asarray(duc2007_docvecs)\n",
    "linkage_matrix = linkage(X, 'average', 'cosine')\n",
    "\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "c, coph_dists = cophenet(linkage_matrix, pdist(X, 'cosine'))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Perform multi-doc summarization at each step\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "# TODO refactor notebook to turn this into a class\n",
    "datasets = {\n",
    "    'duc2006': {\n",
    "        'df': duc2006_df,\n",
    "        'raw_path': DUC2006_RAW_PATH,\n",
    "        'models_path': DUC2006_MODELS_PATH,\n",
    "        'peers_path': DUC2006_PEERS_PATH,\n",
    "        'docvecs': duc2006_docvecs\n",
    "    },\n",
    "    'duc2007': {\n",
    "        'df': duc2007_df,\n",
    "        'raw_path': DUC2007_RAW_PATH,\n",
    "        'models_path': DUC2007_MODELS_PATH,\n",
    "        'peers_path': DUC2007_PEERS_PATH,\n",
    "        'docvecs': duc2007_docvecs\n",
    "    }\n",
    "}\n",
    "\n",
    "# TODO these are just placeholders\n",
    "def get_candidate_sentences(doc_list):\n",
    "    sentences = []\n",
    "    for d in doc_list:\n",
    "        sentences.extend(d.split('.')[0:3])\n",
    "    return sentences\n",
    "def get_candidate_summaries(sentences):\n",
    "    summaries = ['. '.join([random.choice(sentences) for _ in range(3)]) for _ in range(5)]\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1100\n"
     ]
    }
   ],
   "source": [
    "# Compute summaries for each merge of HAC\n",
    "best_summaries = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    # Get the best summaries at each depth (or, e.g., every 100 depths)\n",
    "    N = len(dataset['df'])\n",
    "    best_summaries[dataset_name] = {}\n",
    "    clusters = {i: [i] for i in range(N)}\n",
    "    for i, row in enumerate(linkage_matrix):\n",
    "        # Only compute for every 100 merges, so we get around 10 ROUGE scores (naive)\n",
    "        if i % 100 == 0:\n",
    "            print('Iteration {}'.format(i))\n",
    "        clusters[N+i] = clusters[row[0]] + clusters[row[1]]\n",
    "        doc_list = [r['content'] for _,r in dataset['df'].iloc[clusters[N+i]].iterrows()]\n",
    "        candidates = get_candidate_summaries(get_candidate_sentences(doc_list))\n",
    "        # Evaluate each candidate and pick the best one\n",
    "        cluster_avg = np.mean(X[clusters[N+i]], axis=0)\n",
    "        best_summary = {'score': -1, 'summary': ''}\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            candidate_vec = doc2vec.infer_vector(preprocess(candidate))\n",
    "            candidate_score = cosine(cluster_avg, candidate_vec)\n",
    "            if candidate_score > best_summary['score']:\n",
    "                best_summary['score'] = candidate_score\n",
    "                best_summary['summary'] = candidate\n",
    "        best_summaries[dataset_name][i] = {'score': best_summary['score'], 'summary': best_summary['summary'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open('best_summaries_apnews_duc2007.pkl', 'wb') as f:\n",
    "     pickle.dump(best_summaries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with open('best_summaries_apnews_duc2007.pkl', 'rb') as f:\n",
    "     best_summaries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "datasets['duc2006']['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250, 300)\n",
      "Summarizing topic D0601A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0602B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0603C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0604D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0605E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0606F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0607G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0608H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0609I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0610A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0611B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0612C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0613D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0614E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0615F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0616G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0617H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0618I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0619A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0620B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0621C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0622D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0623E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0624F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0625G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0626H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0627I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0628A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0629B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0630C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0631D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0632E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0633F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0634G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0635H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0636I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0637A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0638B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0639C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0640D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0641E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0642F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0643G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0644H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0645I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0646A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0647B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0648C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0649D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0650E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 300)\n",
      "Summarizing topic D0701A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0702A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0703A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0704A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0705A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0706B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0707B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0708B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0709B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0710C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0711C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0712C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0713C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0714D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0715D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0716D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0717D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0718D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0719E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0720E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0721E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0722E\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0723F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0724F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0725F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0726F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0727G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0728G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0729G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0730G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0731G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0732H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0733H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0734H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0735H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0736H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0737I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0738I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0739I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0740I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0741I\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0742J\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0743J\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0744J\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing topic D0745J\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "# ROUGE (not hierarchical)\n",
    "# Evaluation with ROUGE requires root (not just sudo), so the workflow is:\n",
    "# 1) Place the summaries in each duc dataset's \"peers\" folder\n",
    "# 2) Go to shell as root (using `su`) and run `./ROUGE-1.5.5.pl -n 2 -x -m -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d rougejk.in`\n",
    "# 3) Present the results\n",
    "\n",
    "for dataset_name, dataset in datasets.items():                                           \n",
    "    docvecs = np.concatenate([v.reshape((300, 1)).T for v in dataset['docvecs']])\n",
    "    print(docvecs.shape)\n",
    "    for topic_name, group in dataset['df'].groupby('topic'):\n",
    "        print('Summarizing topic {}'.format(topic_name))\n",
    "        topic_vec = np.mean(docvecs[group.index], axis=0)\n",
    "        doc_list = [r['content'] for _,r in group.iterrows()]\n",
    "        candidates = get_candidate_summaries(get_candidate_sentences(doc_list))\n",
    "        best_summary = {'score': 2, 'summary': ''}\n",
    "        for candidate in candidates:\n",
    "            candidate_vec = doc2vec.infer_vector(preprocess(candidate))\n",
    "            candidate_score = cosine(topic_vec, candidate_vec)\n",
    "            if candidate_score < best_summary['score']:\n",
    "                best_summary['score'] = candidate_score\n",
    "                best_summary['summary'] = candidate \n",
    "        file_path = os.path.join(dataset['peers_path'], '{}.ours'.format(topic_name))\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(best_summary['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create the input files for ROUGE for each dataset\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    topics = {}\n",
    "    for _, _, files in os.walk(dataset['peers_path']):\n",
    "        # TODO update if we have more than one peer\n",
    "        for name in files:\n",
    "            topic = name.split('.')[0][:-1]\n",
    "            topics[topic] = {'peers': name, 'models': -1}\n",
    "    for _, _, files in os.walk(dataset['models_path']):\n",
    "        for t, d in topics.items():\n",
    "            d['models'] = [f for f in files if f.split('.')[0] == t]\n",
    "    # Now write the results\n",
    "    with open('rouge_{}.in'.format(dataset_name), 'w') as f:\n",
    "        f.write('<ROUGE_EVAL version=\"1.5.5\">\\n')\n",
    "        for t, d in topics.items():\n",
    "            f.write('<EVAL ID=\"{}\">\\n'.format(t))\n",
    "            f.write('<PEER-ROOT>\\n{}\\n</PEER-ROOT>\\n'.format(dataset['peers_path']))\n",
    "            f.write('<MODEL-ROOT>\\n{}\\n</MODEL-ROOT>\\n'.format(dataset['models_path']))\n",
    "            f.write('<INPUT-FORMAT TYPE=\"SPL\">\\n')\n",
    "            f.write('</INPUT-FORMAT>\\n')\n",
    "            f.write('<PEERS>\\n')\n",
    "            f.write('<P ID=\"{}\">{}</P>\\n'.format('ours', d['peers']))\n",
    "            f.write('</PEERS>\\n')\n",
    "            f.write('<MODELS>\\n')\n",
    "            for model in d['models']:\n",
    "                f.write('<M ID=\"{}\">{}</M>\\n'.format(model.split('.')[-1], model))\n",
    "            f.write('</MODELS>\\n')\n",
    "            f.write('</EVAL>\\n')\n",
    "        f.write('</ROUGE_EVAL>\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "name": "HierarchicalMultiDocSummarization.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
